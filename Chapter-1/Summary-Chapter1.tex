\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Summary}
\author{sagisk}

\begin{document}
\maketitle

\section{Chapter 1: An Introduction To Neural Networks}

\subsection{The Perceptron:}

Input: $X = \{x_1,...,x_d\}$.

Labels: $y \in \{-1, 1\}$

Weights: $W = \{w_1,...,w_d\}$.

Prediction: $\hat{y} = \text{sign}\{WX\} = \text{sign}\{\sum_{j}w_jx_j\}$

Error: $E(X) = (y - \hat{y}) \in \{-2,0,2\}$

Objective: minimize $L = \sum_{(X,y) \in D}(y-\hat{y})^2 = \sum_{(X,y) \in D}(y-\text{sign}(WX))^2$

For binary classification we can write:

$L_{i}^{0/1} = \frac{1}{2}(y_i-\text{sign}(WX_i))^2 = \frac{1}{2}y_i^2 + \frac{1}{2}\text{sign}(WX_i)^2 - y_i\text{sign}(WX_i) = 1-y_i\text{sign}(WX_i)$
However, since $\text{sign}(WX_i)$ part is dominating in our loss function and we need to make the loss function differentiable we can write:

$L_i = \max\{-y_i(WX_i), 0\}$.

Gradient: Uses a smooth approximation $\nabla_{\text{smooth}}L = \sum_{(X,y)\in D}(y-\hat{y})X$

Weight update: $W = W + aE(X)X = W+a(y-\hat{y})X$

For binary classification:

$W = W - a\nabla_{W}L_i$

Advantages: 
\begin{itemize}
\item Works well when the data is linearly separable.
\end{itemize}

Similarities to other algorithms:


\begin{itemize}
\item SVM (Support Vector Machines): In SVM we  have $L_{i}^{SVM} = \max\{1-y_i(WX_i), 0\}$. In SVM updates on $W$ have the same form as for the Perceptron i.e. $W = W + a\sum_{(X,y) \in S^{+}}yX$. where $S^{+}$ is the set of misclassified training points for which $y(WX_i) < 1$. The main difference between $SVM$ and Perceptron is that Perceptron updates only misclassified points v.s. SVM also updates the points that are correctly classified butt are near the decision boundary.
\end{itemize}

\subsection{Activation functions: }
\begin{itemize}
\item Pre-activation value:  $a_h = WX$
\item Post activation value: $h = \Phi(a_h)$
\end{itemize}

Common functions:
\begin{itemize}
\item Linear: 
\begin{enumerate}
\item $\Phi(v) = v$. 
\item Fields: Regression. 
\item Derivative: $1$.
\end{enumerate}
\item Sign: 
\begin{enumerate}
\item $\Phi(v) = \text{sign}(v)$. 
\item Derivative: If $v\not=0$ then $0$ else non-differentiable.
\end{enumerate}
\item Sigmoid: 
\begin{enumerate}
\item $\Phi(v)  = \frac{1}{1+e^{-v}}$. 
\item Fields: Binary classification to $(0,1)$. 
\item Derivative: $\Phi(v)(1-\Phi(v))$.
\end{enumerate}

\item Tanh: 
\begin{enumerate}
\item $\Phi(v) = \frac{2v-1}{2v+1}$.
\item  Fields: Binary classification to $[-1,1]$. 
\item Derivative: $1-\Phi(v)^2$
\end{enumerate}

\item ReLU (Rectified Linear Unit): 
\begin{enumerate}
\item $\Phi(v) = \max\{v, 0\}$.
\item  Fields: Replace Sigmoid. 
\item Derivative: For non-negative $1$ else $0$.
\end{enumerate}

\item Hard tanh: 
\begin{enumerate}
\item $\Phi(v) = \max\{\min[v,1], -1\}$.  Replace tanh. 
\item Derivative:  For arguments in $[-1,1]$  - $1$ else  $0$.
\end{enumerate}

\end{itemize}
If the multilayer network uses only the  identity activation function in all its layers reduces to a single-layer network performing linear regression. Proof: $h_1 = \Phi(W_1x) = W_1x, ..., h_{p+1} = \Phi(W_{p+1}h_p) = W_{p+1}h_p, o = \Phi(W_{k+1}h_k) = W_{k+1}h_k = W_{k+1}W_{k}...W_{1}x$. Hence, such a network can be represented as a single-layer  network.

\subsection{Loss functions:}
\begin{itemize}
\item Regression: 
\begin{enumerate}
\item Squared loss: $(y-\hat{y})^2$.
\item Hinge loss for $y \in \{-1,1\}$: $\max\{0,1-y\hat{y}\}$
\end{enumerate}
\item Binary targets: $\log(1+e^{-y\hat{y}})$
\item Categorical targets: $-\log(\hat{y_r})$
\end{itemize}

\subsection{Multilayer Networks:}

\begin{itemize}
\item Feed-Forward: 
\begin{itemize}
\item Input-to-hidden layer: $h_1 = \Phi(W_1^Tx)$
\item Hidden-to-hidden layer: $h_{p+1} = \Phi(W_{p+1}^Th_{p})$
\item Hidden-to-output layer: $o = \Phi(W_{k+1}^Th_k)$
\end{itemize}
\item Backpropogation: The main goal of this phase is to learn the gradient. Let the weigth of the connection from hidden unit $h_r $ to $h_{r+1}$ be $w(h_r, h_{r+1})$. Then,
\end{itemize}
$$\frac{dL}{dw(h_{r-1},h_r)}} = \frac{dL}{do}[\sum_{h_r, h_{r+1},...,h_k,o} \frac{do}{dh_k}\prod_{i=1}^{k-1}\frac{dh_{i+1}}{dh_i}]\frac{dh_r}{dw(h_{r-1},h_r)}$$
Where firstly the $\frac{dL}{do}$ is computed. Then the computation goes from later layers to the earlier layers (backwards). So, that while computing the gradient for the earlier layers we already knew the consecutive values for the later values to use them in the computations.

The basic idea of deep learning is that repeated composition of functions can often reduce  the requirements on the number of base functions (computational units) by a factor that  is exponentially related to the number of layers in the network. Or in other word the deeper the network the less units (compared to narrower networks) it will require to solve the task. Moreover, the early layers learn more detailed patterns, wheres the later layers learn higher-level patterns.

\subsection{Practical Issues in Training:}
\begin{enumerate}
\item  Overfitting:
\begin{itemize}
\item When happens:
\begin{enumerate}
\item More parameters then data.
\end{enumerate}
\item Solutions:
\begin{enumerate}
\item Increase the number of training points.
\item Regularization: By adding penalty to the loss function, sharing parameters between nodes (depending on the context like c.n.n.), early stopping (stop training when the error on validation set starts to increase), ensemble methods (bagging: train multiple networks on samples of the training data and average the outputs, dropout: randomly drop nodes from layers with all their corresponding edges).
\end{enumerate}
\end{itemize}
\item Vanishing/Exploding Gradient:
\begin{itemize}
\item  When happens:
\begin{enumerate}
\item Every time (especially in deep networks): To explain suppose we have several layers with one node in each. Then by backpropgation the local derivative of the loss function is given as the product of the derivatives of the activation functions at node times the incoming weight.
\end{enumerate}
\item Solution:
\begin{enumerate}
\item  Some possible solutions regularization and pretraining (see below).
\end{enumerate}
\end{itemize}
\item Local/Spurious Optima:
\begin{itemize}
\item When happens:
\begin{enumerate}
\item Common with higher dimensionalities. Spurious optima are local optima point in the training phase that are not present in the test phase.
\end{enumerate}
\item Solutions:
\begin{enumerate}
\item Pretraining: Greedy, layerwise fashion train the network every layer one at a time. From outer layers to the inner layers. Each layer provides learn the initialization points for the next layer.
\end{enumerate}
\end{itemize}
\end{enumerate}






\bibliographystyle{alpha}
\bibliography{sample}

\end{document}